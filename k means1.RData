

>  wine <- read.csv("G://DataMining/wine.csv")
> wine[1:3,]
  Class Alcohol MalicAcid  Ash AlcalinityAsh Magnesium PhenolsTotal Flavanoids
1     1   14.23      1.71 2.43          15.6       127         2.80       3.06
2     1   13.20      1.78 2.14          11.2       100         2.65       2.76
3     1   13.16      2.36 2.67          18.6       101         2.80       3.24
  NonflavanoidPhenols Proanthocyanins Color  Hue OD280.OD315 Proline
1                0.28            2.29  5.64 1.04        3.92    1065
2                0.26            1.28  4.38 1.05        3.40    1050
3                0.30            2.81  5.68 1.03        3.17    1185
> plot(wine)
> library(tree)
> wine$Class=factor(wine$Class)
> Winetree <- tree(Class ~., data = wine)
> Winetree
node), split, n, deviance, yval, (yprob)
      * denotes terminal node

 1) root 178 386.600 2 ( 0.33146 0.39888 0.26966 )  
   2) Flavanoids < 1.575 62  66.240 3 ( 0.00000 0.22581 0.77419 )  
     4) Color < 3.825 13   0.000 2 ( 0.00000 1.00000 0.00000 ) *
     5) Color > 3.825 49   9.763 3 ( 0.00000 0.02041 0.97959 )  
      10) MalicAcid < 1.675 5   5.004 3 ( 0.00000 0.20000 0.80000 ) *
      11) MalicAcid > 1.675 44   0.000 3 ( 0.00000 0.00000 1.00000 ) *
   3) Flavanoids > 1.575 116 160.800 1 ( 0.50862 0.49138 0.00000 )  
     6) Proline < 724.5 54   9.959 2 ( 0.01852 0.98148 0.00000 )  
      12) Alcohol < 13.08 49   0.000 2 ( 0.00000 1.00000 0.00000 ) *
      13) Alcohol > 13.08 5   5.004 2 ( 0.20000 0.80000 0.00000 ) *
     7) Proline > 724.5 62  29.660 1 ( 0.93548 0.06452 0.00000 )  
      14) Color < 3.55 5   5.004 2 ( 0.20000 0.80000 0.00000 ) *
      15) Color > 3.55 57   0.000 1 ( 1.00000 0.00000 0.00000 ) *
> summary(Winetree)

Classification tree:
tree(formula = Class ~ ., data = wine)
Variables actually used in tree construction:
[1] "Flavanoids" "Color"      "MalicAcid"  "Proline"    "Alcohol"   
Number of terminal nodes:  7 
Residual mean deviance:  0.08779 = 15.01 / 171 
Misclassification error rate: 0.01685 = 3 / 178 
> plot(Winetree, col=8)
> text(Winetree, digits=2)
> set.seed(1)
> cvWine <- cv.tree(Winetree, K=10)
> cvWine$size
[1] 7 6 5 4 3 2 1
> cvWine$dev
[1] 144.7246 156.8253 157.2120 130.6236 176.0941 271.4292 389.3242
> plot(cvWine, pch=21, bg=8, type="p", cex=1.5, ylim=c(100,400))
> Winecut <- prune.tree(Winetree, best=4)
> Winecut
node), split, n, deviance, yval, (yprob)
      * denotes terminal node

1) root 178 386.600 2 ( 0.33146 0.39888 0.26966 )  
  2) Flavanoids < 1.575 62  66.240 3 ( 0.00000 0.22581 0.77419 )  
    4) Color < 3.825 13   0.000 2 ( 0.00000 1.00000 0.00000 ) *
    5) Color > 3.825 49   9.763 3 ( 0.00000 0.02041 0.97959 ) *
  3) Flavanoids > 1.575 116 160.800 1 ( 0.50862 0.49138 0.00000 )  
    6) Proline < 724.5 54   9.959 2 ( 0.01852 0.98148 0.00000 ) *
    7) Proline > 724.5 62  29.660 1 ( 0.93548 0.06452 0.00000 ) *
> summary(Winecut)

Classification tree:
snip.tree(tree = Winetree, nodes = 5:7)
Variables actually used in tree construction:
[1] "Flavanoids" "Color"      "Proline"   
Number of terminal nodes:  4 
Residual mean deviance:  0.2838 = 49.39 / 174 
Misclassification error rate: 0.03371 = 6 / 178 
> plot(Winecut, col=8)
> text(Winecut)
> ## Clustering
> ## standardizing the attributes as units considerably different
> wines=matrix(nrow=length(wine[,1]),ncol=length(wine[1,]))
> for (j in 2:14) {
+ wines[,j]=(wine[,j]-mean(wine[,j]))/sd(wine[,j])
+ }
> wines[,1]=wine[,1]
> winesr=wines[,-1]
> winesr[1:3,]
          [,1]        [,2]       [,3]       [,4]       [,5]      [,6]      [,7]
[1,] 1.5143408 -0.56066822  0.2313998 -1.1663032 1.90852151 0.8067217 1.0319081
[2,] 0.2455968 -0.49800856 -0.8256672 -2.4838405 0.01809398 0.5670481 0.7315653
[3,] 0.1963252  0.02117152  1.1062139 -0.2679823 0.08810981 0.8067217 1.2121137
           [,8]       [,9]      [,10]     [,11]     [,12]     [,13]
[1,] -0.6577078  1.2214385  0.2510088 0.3611585 1.8427215 1.0101594
[2,] -0.8184106 -0.5431887 -0.2924962 0.4049085 1.1103172 0.9625263
[3,] -0.4970050  2.1299594  0.2682629 0.3174085 0.7863692 1.3912237
> ## kmeans clustering with 13 standardized attributes
> grpwines <- kmeans(winesr, centers=3, nstart=20)
> grpwines
K-means clustering with 3 clusters of sizes 65, 51, 62

Cluster means:
        [,1]       [,2]       [,3]       [,4]        [,5]        [,6]
1 -0.9234669 -0.3929331 -0.4931257  0.1701220 -0.49032869 -0.07576891
2  0.1644436  0.8690954  0.1863726  0.5228924 -0.07526047 -0.97657548
3  0.8328826 -0.3029551  0.3636801 -0.6084749  0.57596208  0.88274724
         [,7]        [,8]        [,9]      [,10]      [,11]      [,12]
1  0.02075402 -0.03343924  0.05810161 -0.8993770  0.4605046  0.2700025
2 -1.21182921  0.72402116 -0.77751312  0.9388902 -1.1615122 -1.2887761
3  0.97506900 -0.56050853  0.57865427  0.1705823  0.4726504  0.7770551
       [,13]
1 -0.7517257
2 -0.4059428
3  1.1220202

Clustering vector:
  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 3
 [75] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[112] 1 1 1 1 1 1 1 2 1 1 3 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2

Within cluster sum of squares by cluster:
[1] 558.6971 326.3537 385.6983
 (between_SS / total_SS =  44.8 %)

Available components:

[1] "cluster"      "centers"      "totss"        "withinss"     "tot.withinss"
[6] "betweenss"    "size"         "iter"         "ifault"      
> grpwines$cluster
  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
 [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 3
 [75] 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 3 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
[112] 1 1 1 1 1 1 1 2 1 1 3 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[149] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
> wine$Class ## actual classes
  [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
 [38] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
 [75] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
[112] 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
[149] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3
Levels: 1 2 3
> ## 6 mistakes made among 178 wines
> library(MASS)
> wines[1:3,]
     [,1]      [,2]        [,3]       [,4]       [,5]       [,6]      [,7]
[1,]    1 1.5143408 -0.56066822  0.2313998 -1.1663032 1.90852151 0.8067217
[2,]    1 0.2455968 -0.49800856 -0.8256672 -2.4838405 0.01809398 0.5670481
[3,]    1 0.1963252  0.02117152  1.1062139 -0.2679823 0.08810981 0.8067217
          [,8]       [,9]      [,10]      [,11]     [,12]     [,13]     [,14]
[1,] 1.0319081 -0.6577078  1.2214385  0.2510088 0.3611585 1.8427215 1.0101594
[2,] 0.7315653 -0.8184106 -0.5431887 -0.2924962 0.4049085 1.1103172 0.9625263
[3,] 1.2121137 -0.4970050  2.1299594  0.2682629 0.3174085 0.7863692 1.3912237
> ws=data.frame(wines)
> ws[1:3,]
  X1        X2          X3         X4         X5         X6        X7        X8
1  1 1.5143408 -0.56066822  0.2313998 -1.1663032 1.90852151 0.8067217 1.0319081
2  1 0.2455968 -0.49800856 -0.8256672 -2.4838405 0.01809398 0.5670481 0.7315653
3  1 0.1963252  0.02117152  1.1062139 -0.2679823 0.08810981 0.8067217 1.2121137
          X9        X10        X11       X12       X13       X14
1 -0.6577078  1.2214385  0.2510088 0.3611585 1.8427215 1.0101594
2 -0.8184106 -0.5431887 -0.2924962 0.4049085 1.1103172 0.9625263
3 -0.4970050  2.1299594  0.2682629 0.3174085 0.7863692 1.3912237
> zlin=lda(X1~.,ws,prior=c(1,1,1)/3)
> zlin
Call:
lda(X1 ~ ., data = ws, prior = c(1, 1, 1)/3)

Prior probabilities of groups:
        1         2         3 
0.3333333 0.3333333 0.3333333 

Group means:
          X2         X3         X4         X5          X6          X7
1  0.9166093 -0.2915199  0.3246886 -0.7359212  0.46192317  0.87090552
2 -0.8892116 -0.3613424 -0.4437061  0.2225094 -0.36354162 -0.05790375
3  0.1886265  0.8928122  0.2572190  0.5754413 -0.03004191 -0.98483874
           X8          X9        X10        X11        X12        X13
1  0.95419225 -0.57735640  0.5388633  0.2028288  0.4575567  0.7691811
2  0.05163434  0.01452785  0.0688079 -0.8503999  0.4323908  0.2446043
3 -1.24923710  0.68817813 -0.7641311  1.0085728 -1.2019916 -1.3072623
         X14
1  1.1711967
2 -0.7220731
3 -0.3715295

Coefficients of linear discriminants:
            LD1          LD2
X2  -0.28930985  0.724190975
X3   0.20253117  0.330831040
X4  -0.06681410  0.648051143
X5   0.49017234 -0.515701775
X6  -0.03120840 -0.004953265
X7   0.38518458 -0.040744671
X8  -1.68312608 -0.402314982
X9  -0.19671169 -0.192768856
X10  0.06727363 -0.179604486
X11  0.85323534  0.542363156
X12 -0.20517529 -0.335974619
X13 -0.81875175  0.080084913
X14 -0.79840001  0.942311575

Proportion of trace:
   LD1    LD2 
0.7298 0.2702 
> ## quadratic discriminant analysis
> zqua=qda(X1~.,ws,prior=c(1,1,1)/3)
> zqua
Call:
qda(X1 ~ ., data = ws, prior = c(1, 1, 1)/3)

Prior probabilities of groups:
        1         2         3 
0.3333333 0.3333333 0.3333333 

Group means:
          X2         X3         X4         X5          X6          X7
1  0.9166093 -0.2915199  0.3246886 -0.7359212  0.46192317  0.87090552
2 -0.8892116 -0.3613424 -0.4437061  0.2225094 -0.36354162 -0.05790375
3  0.1886265  0.8928122  0.2572190  0.5754413 -0.03004191 -0.98483874
           X8          X9        X10        X11        X12        X13
1  0.95419225 -0.57735640  0.5388633  0.2028288  0.4575567  0.7691811
2  0.05163434  0.01452785  0.0688079 -0.8503999  0.4323908  0.2446043
3 -1.24923710  0.68817813 -0.7641311  1.0085728 -1.2019916 -1.3072623
         X14
1  1.1711967
2 -0.7220731
3 -0.3715295
> n=dim(ws)[1]
> errorlin=1-(sum(ws$X1==predict(zlin,ws)$class)/n)
> errorlin
[1] 0
> errorqua=1-(sum(ws$X1==predict(zqua,ws)$class)/n)
> errorqua
[1] 0.005617978
> neval=1
> corlin=dim(n)
> corqua=dim(n)
> ## leave one out evaluation
> for (k in 1:n) {
+ train1=c(1:n)
+ train=train1[train1!=k]
+ zlin=lda(X1~.,ws[train,],prior=c(1,1,1)/3)
+ corlin[k]=ws$X1[-train]==predict(zlin,ws[-train,])$class
+ zqua=qda(X1~.,ws[train,],prior=c(1,1,1)/3)
+ corqua[k]=ws$X1[-train]==predict(zqua,ws[-train,])$class
+ }
> merrlin=1-mean(corlin)
> merrlin
[1] 0.01123596
> merrqua=1-mean(corqua)
> merrqua
[1] 0.005617978
> ## quadratic discriminant analysis past structure
> library(VGAM)
Error in library(VGAM) : there is no package called ‘VGAM’
> setRepositories()
> utils:::menuInstallPkgs()
--- Please select a CRAN mirror for use in this session ---
trying URL 'http://cran.um.ac.ir/bin/windows/contrib/3.2/VGAM_1.0-2.zip'
Content type 'application/zip' length 5392648 bytes (5.1 MB)
downloaded 5.1 MB

package ‘VGAM’ successfully unpacked and MD5 sums checked

The downloaded binary packages are in
        C:\Documents and Settings\farzad\Local Settings\Temp\Rtmp6dnks6\downloaded_packages
> library(VGAM)
Loading required package: stats4
Loading required package: splines

Attaching package: ‘VGAM’

The following object is masked _by_ ‘.GlobalEnv’:

    wine

> ws=data.frame(wines)
> gg <- vglm(X1 ~ .,multinomial,data=ws)
There were 44 warnings (use warnings() to see them)
> summary(gg)

Call:
vglm(formula = X1 ~ ., family = multinomial, data = ws)

Pearson residuals:
                          Min         1Q     Median        3Q       Max
log(mu[,1]/mu[,3]) -3.535e-06 -4.037e-08 -4.189e-09 4.928e-08 4.069e-06
log(mu[,2]/mu[,3]) -3.728e-06 -5.521e-08  2.151e-08 6.559e-08 3.199e-06

Coefficients:
               Estimate Std. Error z value Pr(>|z|)
(Intercept):1     1.706   7040.396   0.000    1.000
(Intercept):2    -2.428   6921.540   0.000    1.000
X2:1              2.405  10867.383   0.000    1.000
X2:2             -8.468   8973.144  -0.001    0.999
X3:1              4.052   5856.344   0.001    0.999
X3:2             -5.252   5110.788  -0.001    0.999
X4:1             -1.531   9461.716   0.000    1.000
X4:2            -15.013   9118.403  -0.002    0.999
X5:1             -6.322   7541.958  -0.001    0.999
X5:2              5.787   6510.903   0.001    0.999
X6:1              6.970   9051.704   0.001    0.999
X6:2              6.657   7570.096   0.001    0.999
X7:1             -7.558   9078.772  -0.001    0.999
X7:2             -3.045   6589.816   0.000    1.000
X8:1             19.678  14327.763   0.001    0.999
X8:2             14.752  13533.494   0.001    0.999
X9:1              7.033   6829.084   0.001    0.999
X9:2             11.692   5113.602   0.002    0.998
X10:1             4.699   8636.751   0.001    1.000
X10:2             6.595   7979.541   0.001    0.999
X11:1           -21.383  12345.138  -0.002    0.999
X11:2           -33.030   5595.380  -0.006    0.995
X12:1             3.940  11822.084   0.000    1.000
X12:2            13.367  11849.069   0.001    0.999
X13:1            10.746   9648.257   0.001    0.999
X13:2             6.413   5937.106   0.001    0.999
X14:1            10.612  11455.359   0.001    0.999
X14:2           -20.993   9746.104  -0.002    0.998

Number of linear predictors:  2 

Names of linear predictors: log(mu[,1]/mu[,3]), log(mu[,2]/mu[,3])

Dispersion Parameter for multinomial family:   1

Residual deviance: 4.801e-07 on 328 degrees of freedom

Log-likelihood: -2.4e-07 on 328 degrees of freedom

Number of iterations: 21 

Reference group is level  3  of the response
> predict(gg)
    log(mu[,1]/mu[,3]) log(mu[,2]/mu[,3])
1           65.2978902         -9.5455178
2           49.1684531         -5.8475443
3           45.8746205        -26.8928063
4           33.5594066        -97.4058657
5           29.6271939          9.0465051
6           47.0158939        -64.2698408
7           50.2623811        -51.1354322
8           51.8003786        -44.1644975
9           40.9818405        -21.5400891
10          20.1099266        -44.5898762
11          58.8978613        -36.5010216
12          33.3960816        -38.2407943
13          33.3065579        -47.8150947
14          69.7346722         -9.7180340
15          60.5871776        -69.8085119
16          24.1253788        -68.2359325
17          38.0522707        -52.6069675
18          29.8017885        -29.0248691
19          42.4450805        -92.7967368
20          40.1957950        -27.7865407
21          48.0146731         15.6275854
22          31.9321909         -6.6858825
23          61.7463836         12.0629760
24          33.4148870         -2.7365699
25          40.2658878         16.9785307
26          43.3726449         25.4015594
27          37.4190302        -42.5403278
28          30.1016215        -21.7948740
29          44.2626609          4.4470113
30          36.3274811        -12.0216064
31          29.5613177        -32.1478824
32          31.0245287        -64.7689965
33          54.2834489         29.6329652
34          51.0413642         -9.0438438
35          36.3675449        -17.3281458
36          29.3530021          0.5747289
37          29.4858387        -19.7810022
38          21.8282107        -20.2549874
39          38.1552295         20.7707990
40          55.2823523        -23.7025670
41          38.7290668          9.1725449
42          33.1021318        -14.4597945
43          40.5646254        -52.2087428
44          27.7348731          3.5299495
45          33.8015860         14.8311143
46          35.8236289        -49.5520331
47          55.3460502        -19.1477986
48          32.0757540        -19.4667907
49          26.7533457        -27.1348580
50          27.2196905        -57.7326882
51          31.5708544        -29.8833416
52          45.5015261        -38.0046917
53          39.3697796        -47.5705599
54          38.9329770        -57.7178852
55          33.5340974        -29.0708589
56          19.6401902        -35.4193575
57          29.9786213        -28.5373613
58          32.8039591        -53.9149027
59          26.8941233        -68.3847436
60         -17.0490388         61.3778637
61          -2.1081115         47.0878593
62         -34.5500216         19.1622934
63           0.5457576         38.7144050
64          -6.5145925         59.4917438
65           6.8543914         83.7664419
66          12.8555058         32.4933586
67          14.0568736         60.7693337
68          -9.3147299         48.1345685
69          -0.4906975         17.8760486
70          46.1954770        101.2905371
71           1.4539258         20.4612473
72           3.9657425         51.9131752
73         -16.0173279         28.5127802
74          39.8011346         59.4557351
75          -1.1576525         20.0205203
76          -3.3615384         71.0813474
77          -9.3403212         58.8122229
78          13.9612533         59.0992237
79          46.5114768         80.7764567
80          36.0059896         86.6103241
81          10.8747579        119.6046817
82          22.2939253         40.1395705
83           0.1071609         68.9611034
84          -8.4109726         17.9062096
85           9.8623642         36.2542473
86          19.2831374         78.6299429
87           5.6888593         90.1379966
88           4.6351838         79.3399394
89           8.0691626         49.6200006
90           4.9584361         73.1910045
91           9.9121502         72.3734707
92          -7.3992186         59.2590022
93           0.6713163         60.7601677
94          23.9073650         88.5845143
95          -6.3065576         61.5597064
96          76.0418104         96.6688952
97          -2.5985469         17.8087703
98          17.2250588         87.6933983
99          14.0928008         49.6369696
100         53.0437167        126.4292361
101         26.0756420         79.6789640
102          8.2904498         65.2102071
103         10.1259486         45.7187711
104         -5.5433373         87.3319551
105         19.7750725         63.8545758
106         26.8846076         96.8493607
107         15.0435717         62.9949201
108          1.8606253         53.9310649
109         11.7670826         98.2079133
110         33.7012121         56.0389195
111         31.5228869         88.3766670
112          1.5507001         68.3627836
113         22.3377465         41.9873024
114        -13.0915045         68.6922210
115         -1.3114347         63.2959001
116         30.5290250        162.6885285
117         10.2685957         86.2281971
118         21.5123214        104.9093782
119        -14.8070884         19.2089046
120         32.9656517         84.1714646
121         23.1794079         48.4762791
122         39.4144888         58.9061725
123         23.0047963         67.1011659
124         27.2626025         47.9803674
125         29.4707494         59.8925122
126         13.5946112         77.9439123
127          2.6350804         56.3425676
128          1.2381498         74.1279972
129         11.1733624         94.4628366
130          3.2254184         28.2521498
131        -26.6409854        -17.4917060
132        -44.6508363        -44.4342622
133        -60.9221590        -46.2394964
134        -49.1214021        -45.9731790
135        -47.9507313        -18.8396265
136        -48.1532386        -41.6193064
137        -31.2353950        -34.8346681
138        -36.4821536        -18.8027700
139        -38.9984435        -30.3196970
140        -43.3101749        -18.7353297
141        -30.2520036        -31.4768214
142        -39.6529061        -65.1429808
143        -36.7237948        -26.8413796
144        -20.7345817        -21.4896953
145        -47.4330234        -90.3167099
146        -19.6669918        -29.0943329
147        -54.0635336        -59.9036789
148        -68.0318178        -96.6383526
149        -76.4409616        -97.7226994
150        -69.2791060        -95.9524483
151        -68.7931512        -93.5704390
152        -97.5101221       -121.3863870
153        -79.8164319        -64.5775859
154        -70.2643381       -102.1494045
155        -61.6102756        -40.7391876
156        -52.6317659        -79.1478784
157        -72.7210657        -99.8144681
158        -63.1460491        -71.1100519
159       -104.2240905       -143.2459092
160       -104.3283268       -128.4133585
161        -76.0765413        -70.5120463
162        -35.0717812        -49.0210568
163        -30.1957993        -17.8274915
164        -30.0839566        -46.6657439
165        -85.3184275       -105.4853249
166        -50.9135359        -47.5893126
167        -77.7238031       -115.9990478
168        -87.7240048       -118.7146199
169        -66.8171275        -99.6516393
170        -73.6012424       -121.7222418
171        -50.4113950        -36.3389723
172       -100.6738821       -101.8827902
173        -82.5175387       -126.3126889
174        -48.0765426       -103.2916794
175        -55.3565715        -81.9476181
176        -63.6279936       -121.0739965
177        -55.4844579        -94.2668430
178        -80.5655333       -113.7352797
> round(fitted(gg),2) ## probabilities
    1 2 3
1   1 0 0
2   1 0 0
3   1 0 0
4   1 0 0
5   1 0 0
6   1 0 0
7   1 0 0
8   1 0 0
9   1 0 0
10  1 0 0
11  1 0 0
12  1 0 0
13  1 0 0
14  1 0 0
15  1 0 0
16  1 0 0
17  1 0 0
18  1 0 0
19  1 0 0
20  1 0 0
21  1 0 0
22  1 0 0
23  1 0 0
24  1 0 0
25  1 0 0
26  1 0 0
27  1 0 0
28  1 0 0
29  1 0 0
30  1 0 0
31  1 0 0
32  1 0 0
33  1 0 0
34  1 0 0
35  1 0 0
36  1 0 0
37  1 0 0
38  1 0 0
39  1 0 0
40  1 0 0
41  1 0 0
42  1 0 0
43  1 0 0
44  1 0 0
45  1 0 0
46  1 0 0
47  1 0 0
48  1 0 0
49  1 0 0
50  1 0 0
51  1 0 0
52  1 0 0
53  1 0 0
54  1 0 0
55  1 0 0
56  1 0 0
57  1 0 0
58  1 0 0
59  1 0 0
60  0 1 0
61  0 1 0
62  0 1 0
63  0 1 0
64  0 1 0
65  0 1 0
66  0 1 0
67  0 1 0
68  0 1 0
69  0 1 0
70  0 1 0
71  0 1 0
72  0 1 0
73  0 1 0
74  0 1 0
75  0 1 0
76  0 1 0
77  0 1 0
78  0 1 0
79  0 1 0
80  0 1 0
81  0 1 0
82  0 1 0
83  0 1 0
84  0 1 0
85  0 1 0
86  0 1 0
87  0 1 0
88  0 1 0
89  0 1 0
90  0 1 0
91  0 1 0
92  0 1 0
93  0 1 0
94  0 1 0
95  0 1 0
96  0 1 0
97  0 1 0
98  0 1 0
99  0 1 0
100 0 1 0
101 0 1 0
102 0 1 0
103 0 1 0
104 0 1 0
105 0 1 0
106 0 1 0
107 0 1 0
108 0 1 0
109 0 1 0
110 0 1 0
111 0 1 0
112 0 1 0
113 0 1 0
114 0 1 0
115 0 1 0
116 0 1 0
117 0 1 0
118 0 1 0
119 0 1 0
120 0 1 0
121 0 1 0
122 0 1 0
123 0 1 0
124 0 1 0
125 0 1 0
126 0 1 0
127 0 1 0
128 0 1 0
129 0 1 0
130 0 1 0
131 0 0 1
132 0 0 1
133 0 0 1
134 0 0 1
135 0 0 1
136 0 0 1
137 0 0 1
138 0 0 1
139 0 0 1
140 0 0 1
141 0 0 1
142 0 0 1
143 0 0 1
144 0 0 1
145 0 0 1
146 0 0 1
147 0 0 1
148 0 0 1
149 0 0 1
150 0 0 1
151 0 0 1
152 0 0 1
153 0 0 1
154 0 0 1
155 0 0 1
156 0 0 1
157 0 0 1
158 0 0 1
159 0 0 1
160 0 0 1
161 0 0 1
162 0 0 1
163 0 0 1
164 0 0 1
165 0 0 1
166 0 0 1
167 0 0 1
168 0 0 1
169 0 0 1
170 0 0 1
171 0 0 1
172 0 0 1
173 0 0 1
174 0 0 1
175 0 0 1
176 0 0 1
177 0 0 1
178 0 0 1
> cbind(round(fitted(gg),2),ws$X1)
    1 2 3  
1   1 0 0 1
2   1 0 0 1
3   1 0 0 1
4   1 0 0 1
5   1 0 0 1
6   1 0 0 1
7   1 0 0 1
8   1 0 0 1
9   1 0 0 1
10  1 0 0 1
11  1 0 0 1
12  1 0 0 1
13  1 0 0 1
14  1 0 0 1
15  1 0 0 1
16  1 0 0 1
17  1 0 0 1
18  1 0 0 1
19  1 0 0 1
20  1 0 0 1
21  1 0 0 1
22  1 0 0 1
23  1 0 0 1
24  1 0 0 1
25  1 0 0 1
26  1 0 0 1
27  1 0 0 1
28  1 0 0 1
29  1 0 0 1
30  1 0 0 1
31  1 0 0 1
32  1 0 0 1
33  1 0 0 1
34  1 0 0 1
35  1 0 0 1
36  1 0 0 1
37  1 0 0 1
38  1 0 0 1
39  1 0 0 1
40  1 0 0 1
41  1 0 0 1
42  1 0 0 1
43  1 0 0 1
44  1 0 0 1
45  1 0 0 1
46  1 0 0 1
47  1 0 0 1
48  1 0 0 1
49  1 0 0 1
50  1 0 0 1
51  1 0 0 1
52  1 0 0 1
53  1 0 0 1
54  1 0 0 1
55  1 0 0 1
56  1 0 0 1
57  1 0 0 1
58  1 0 0 1
59  1 0 0 1
60  0 1 0 2
61  0 1 0 2
62  0 1 0 2
63  0 1 0 2
64  0 1 0 2
65  0 1 0 2
66  0 1 0 2
67  0 1 0 2
68  0 1 0 2
69  0 1 0 2
70  0 1 0 2
71  0 1 0 2
72  0 1 0 2
73  0 1 0 2
74  0 1 0 2
75  0 1 0 2
76  0 1 0 2
77  0 1 0 2
78  0 1 0 2
79  0 1 0 2
80  0 1 0 2
81  0 1 0 2
82  0 1 0 2
83  0 1 0 2
84  0 1 0 2
85  0 1 0 2
86  0 1 0 2
87  0 1 0 2
88  0 1 0 2
89  0 1 0 2
90  0 1 0 2
91  0 1 0 2
92  0 1 0 2
93  0 1 0 2
94  0 1 0 2
95  0 1 0 2
96  0 1 0 2
97  0 1 0 2
98  0 1 0 2
99  0 1 0 2
100 0 1 0 2
101 0 1 0 2
102 0 1 0 2
103 0 1 0 2
104 0 1 0 2
105 0 1 0 2
106 0 1 0 2
107 0 1 0 2
108 0 1 0 2
109 0 1 0 2
110 0 1 0 2
111 0 1 0 2
112 0 1 0 2
113 0 1 0 2
114 0 1 0 2
115 0 1 0 2
116 0 1 0 2
117 0 1 0 2
118 0 1 0 2
119 0 1 0 2
120 0 1 0 2
121 0 1 0 2
122 0 1 0 2
123 0 1 0 2
124 0 1 0 2
125 0 1 0 2
126 0 1 0 2
127 0 1 0 2
128 0 1 0 2
129 0 1 0 2
130 0 1 0 2
131 0 0 1 3
132 0 0 1 3
133 0 0 1 3
134 0 0 1 3
135 0 0 1 3
136 0 0 1 3
137 0 0 1 3
138 0 0 1 3
139 0 0 1 3
140 0 0 1 3
141 0 0 1 3
142 0 0 1 3
143 0 0 1 3
144 0 0 1 3
145 0 0 1 3
146 0 0 1 3
147 0 0 1 3
148 0 0 1 3
149 0 0 1 3
150 0 0 1 3
151 0 0 1 3
152 0 0 1 3
153 0 0 1 3
154 0 0 1 3
155 0 0 1 3
156 0 0 1 3
157 0 0 1 3
158 0 0 1 3
159 0 0 1 3
160 0 0 1 3
161 0 0 1 3
162 0 0 1 3
163 0 0 1 3
164 0 0 1 3
165 0 0 1 3
166 0 0 1 3
167 0 0 1 3
168 0 0 1 3
169 0 0 1 3
170 0 0 1 3
171 0 0 1 3
172 0 0 1 3
173 0 0 1 3
174 0 0 1 3
175 0 0 1 3
176 0 0 1 3
177 0 0 1 3
178 0 0 1 3
> ## perfect classification
> 
